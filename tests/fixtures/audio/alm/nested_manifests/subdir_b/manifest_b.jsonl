{"audio_filepath": "/data/audio/lecture_003.wav", "audio_sample_rate": 16000, "duration": 645.2, "segments": [{"text": "Today we discuss deep learning architectures in detail", "start": 0.0, "end": 4.8, "speaker": "professor", "metrics": {"bandwidth": 8600}}, {"text": "Can you explain the historical context first", "start": 5.5, "end": 8.9, "speaker": "student_a", "metrics": {"bandwidth": 8450}}, {"text": "Neural networks have revolutionized artificial intelligence", "start": 9.2, "end": 13.8, "speaker": "professor", "metrics": {"bandwidth": 8450}}, {"text": "Let's start with the basics of perceptrons", "start": 14.5, "end": 18.2, "speaker": "professor", "metrics": {"bandwidth": 8700}}, {"text": "How does a perceptron differ from biological neurons", "start": 18.8, "end": 22.9, "speaker": "student_b", "metrics": {"bandwidth": 8550}}, {"text": "A perceptron is the simplest neural network unit", "start": 23.5, "end": 27.8, "speaker": "professor", "metrics": {"bandwidth": 8550}}, {"text": "It takes multiple inputs and produces single output", "start": 28.3, "end": 32.6, "speaker": "professor", "metrics": {"bandwidth": 8800}}, {"text": "What about the mathematical formulation", "start": 33.1, "end": 36.2, "speaker": "student_a", "metrics": {"bandwidth": 8400}}, {"text": "Activation functions determine output characteristics", "start": 36.8, "end": 40.9, "speaker": "professor", "metrics": {"bandwidth": 8400}}, {"text": "Common functions include sigmoid, tanh, and ReLU", "start": 41.4, "end": 45.9, "speaker": "professor", "metrics": {"bandwidth": 8650}}, {"text": "ReLU has become popular due to computational efficiency", "start": 46.5, "end": 51.0, "speaker": "professor", "metrics": {"bandwidth": 8750}}, {"text": "Now let's explore multi-layer perceptron networks", "start": 120.0, "end": 124.6, "speaker": "professor", "metrics": {"bandwidth": 8500}}, {"text": "Why do we need multiple layers", "start": 125.0, "end": 127.8, "speaker": "student_b", "metrics": {"bandwidth": 8350}}, {"text": "Hidden layers enable learning complex patterns", "start": 128.4, "end": 132.1, "speaker": "professor", "metrics": {"bandwidth": 8350}}, {"text": "Backpropagation algorithm trains these networks effectively", "start": 132.7, "end": 137.5, "speaker": "professor", "metrics": {"bandwidth": 8850}}, {"text": "Can you show us the gradient calculation", "start": 138.0, "end": 141.2, "speaker": "student_a", "metrics": {"bandwidth": 8300}}, {"text": "Gradient descent optimizes the network parameters", "start": 141.8, "end": 146.1, "speaker": "professor", "metrics": {"bandwidth": 8300}}, {"text": "Learning rate controls the optimization step size", "start": 146.7, "end": 150.8, "speaker": "professor", "metrics": {"bandwidth": 8600}}, {"text": "Too high learning rates cause unstable training", "start": 151.4, "end": 155.5, "speaker": "professor", "metrics": {"bandwidth": 8450}}, {"text": "What happens with very small learning rates", "start": 156.0, "end": 159.3, "speaker": "student_b", "metrics": {"bandwidth": 8700}}, {"text": "Too low rates result in very slow convergence", "start": 159.8, "end": 163.9, "speaker": "professor", "metrics": {"bandwidth": 8700}}, {"text": "Adaptive learning rate methods help optimize training", "start": 164.4, "end": 169.1, "speaker": "professor", "metrics": {"bandwidth": 8550}}, {"text": "Convolutional neural networks excel at image processing", "start": 240.0, "end": 244.9, "speaker": "professor", "metrics": {"bandwidth": 8800}}, {"text": "How do convolutions work exactly", "start": 245.4, "end": 248.5, "speaker": "student_a", "metrics": {"bandwidth": 8400}}, {"text": "Convolution operations detect local features efficiently", "start": 249.0, "end": 253.5, "speaker": "professor", "metrics": {"bandwidth": 8400}}, {"text": "Pooling layers reduce spatial dimensions systematically", "start": 254.1, "end": 258.6, "speaker": "professor", "metrics": {"bandwidth": 8650}}, {"text": "What about feature maps and channels", "start": 259.1, "end": 262.2, "speaker": "student_b", "metrics": {"bandwidth": 8750}}, {"text": "Multiple convolution layers create hierarchical features", "start": 262.7, "end": 267.5, "speaker": "professor", "metrics": {"bandwidth": 8750}}, {"text": "Recurrent networks handle sequential data processing", "start": 268.0, "end": 272.5, "speaker": "professor", "metrics": {"bandwidth": 8500}}, {"text": "LSTM cells solve vanishing gradient problems", "start": 273.0, "end": 277.1, "speaker": "professor", "metrics": {"bandwidth": 8350}}, {"text": "Can you explain the gates in LSTM", "start": 277.6, "end": 280.8, "speaker": "student_a", "metrics": {"bandwidth": 8850}}, {"text": "Attention mechanisms focus on relevant information", "start": 281.3, "end": 285.8, "speaker": "professor", "metrics": {"bandwidth": 8850}}, {"text": "Transformer architecture uses self-attention exclusively", "start": 360.0, "end": 364.8, "speaker": "professor", "metrics": {"bandwidth": 8300}}, {"text": "How does self-attention differ from regular attention", "start": 365.3, "end": 369.1, "speaker": "student_b", "metrics": {"bandwidth": 8600}}, {"text": "Multi-head attention processes different representation subspaces", "start": 369.6, "end": 374.9, "speaker": "professor", "metrics": {"bandwidth": 8600}}, {"text": "Positional encoding provides sequence order information", "start": 375.4, "end": 379.9, "speaker": "professor", "metrics": {"bandwidth": 8450}}, {"text": "Why is positional encoding necessary", "start": 380.4, "end": 383.6, "speaker": "student_a", "metrics": {"bandwidth": 8700}}, {"text": "Layer normalization stabilizes training dynamics", "start": 384.1, "end": 388.2, "speaker": "professor", "metrics": {"bandwidth": 8700}}, {"text": "Residual connections help with gradient flow", "start": 388.7, "end": 393.0, "speaker": "professor", "metrics": {"bandwidth": 8550}}, {"text": "Dropout regularization prevents model overfitting", "start": 480.0, "end": 484.5, "speaker": "professor", "metrics": {"bandwidth": 8800}}, {"text": "What dropout rate should we typically use", "start": 485.0, "end": 488.2, "speaker": "student_b", "metrics": {"bandwidth": 8400}}, {"text": "Batch normalization accelerates training convergence", "start": 488.7, "end": 493.2, "speaker": "professor", "metrics": {"bandwidth": 8400}}, {"text": "Data augmentation increases dataset diversity artificially", "start": 493.8, "end": 498.6, "speaker": "professor", "metrics": {"bandwidth": 8650}}, {"text": "What types of augmentation work best", "start": 499.1, "end": 502.3, "speaker": "student_a", "metrics": {"bandwidth": 8750}}, {"text": "Transfer learning leverages pre-trained model knowledge", "start": 502.8, "end": 507.6, "speaker": "professor", "metrics": {"bandwidth": 8750}}, {"text": "Fine-tuning adapts models to specific tasks", "start": 508.1, "end": 512.2, "speaker": "professor", "metrics": {"bandwidth": 8500}}, {"text": "Model interpretation remains an active research area", "start": 600.0, "end": 605.2, "speaker": "professor", "metrics": {"bandwidth": 8350}}, {"text": "Why is interpretability so important", "start": 605.7, "end": 608.9, "speaker": "student_b", "metrics": {"bandwidth": 8850}}, {"text": "Gradient-based methods visualize important input features", "start": 609.4, "end": 614.5, "speaker": "professor", "metrics": {"bandwidth": 8850}}, {"text": "LIME and SHAP provide local explanation techniques", "start": 615.0, "end": 619.7, "speaker": "professor", "metrics": {"bandwidth": 8300}}, {"text": "Next week we'll cover generative adversarial networks", "start": 640.0, "end": 644.8, "speaker": "professor", "metrics": {"bandwidth": 8600}}]}
